{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d, ReLU, Softmax, CrossEntropyLoss, Sequential, Dropout, Conv2d, Linear\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear, QuantReLU\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from tensor_norm import TensorNorm\n",
    "from common import CommonWeightQuant, CommonActQuant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNV_OUT_CH_POOL = [(21, False), (21, True), (21, False)] #[(21, False), (21, True), (21, False)]\n",
    "INTERMEDIATE_FC_FEATURES = [(3549, 16), (16, 16)] #[(3549, 16), (16, 16)]\n",
    "LAST_FC_IN_FEATURES = 16\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "POOL_SIZE = 2\n",
    "KERNEL_SIZE = 6\n",
    "\n",
    "MixPrecisionBits = 4  \n",
    "ConvPrecisonBits = 4  \n",
    "\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes, weight_bit_width, act_bit_width, in_bit_width, in_ch):\n",
    "        super(CNV, self).__init__()\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        self.conv_features.append(QuantIdentity( # for Q1.7 input format\n",
    "            act_quant=CommonActQuant,\n",
    "            bit_width=in_bit_width,\n",
    "            min_val=- 1.0,\n",
    "            max_val=1.0 - 2.0 ** (-7),\n",
    "            narrow_range=True,  ###If True implements the value in a range he range for weights and biases\n",
    "            #will be from -2^(N-1) + 1 to 2^(N-1), where N is the bit width. This is different from the default \n",
    "            #range of -2^(N-1) to 2^(N-1) when narrow_range is False.\n",
    "            #narrow_range = True makes the hardware inference more efficient\n",
    "            restrict_scaling_type=RestrictValueType.POWER_OF_TWO))\n",
    "\n",
    "        for out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(QuantConv2d(kernel_size=KERNEL_SIZE, in_channels=in_ch, out_channels=out_ch,\n",
    "                bias=True, padding=4, weight_quant=CommonWeightQuant, weight_bit_width=ConvPrecisonBits))#made bias=False\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm2d(in_ch, eps=1e-4))\n",
    "            self.conv_features.append(QuantIdentity(act_quant=CommonActQuant,bit_width=MixPrecisionBits))#MultiThreshold123\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool2d(kernel_size=2))\n",
    "\n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(QuantLinear(in_features=in_features, out_features=out_features, bias=True,\n",
    "                weight_quant=CommonWeightQuant, weight_bit_width=weight_bit_width))\n",
    "            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))\n",
    "            self.linear_features.append(QuantIdentity(act_quant=CommonActQuant,bit_width=act_bit_width))#MultiThreshold45\n",
    "\n",
    "        self.linear_features.append(QuantLinear(in_features=LAST_FC_IN_FEATURES, out_features=num_classes, bias=False,\n",
    "            weight_quant=CommonWeightQuant, weight_bit_width=weight_bit_width))\n",
    "        self.linear_features.append(TensorNorm())\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "                #print(f\"Weight Data Convolution {m+1}\", m.weight.data)\n",
    "\n",
    "\n",
    "    def clip_weights(self, min_val, max_val):\n",
    "        for mod in self.conv_features:\n",
    "            if isinstance(mod, QuantConv2d):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "                #print(f\"Weight Data Convolution {mod+1}\", mod.weight.data)\n",
    "        for mod in self.linear_features:\n",
    "            if isinstance(mod, QuantLinear):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "                #print(f\"Weight Data Convolution {mod+1}\", mod.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Data Feeded:\",x)\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        #print(\"Data Processesd(2x-1):\",x)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "            #print(f\"Data After Convolution Feature {mod+1}:\",x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(\"Data After Flatten:\",x)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "            #print(f\"Data After Linear Feature {mod+1}:\",x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNV(num_classes=5, weight_bit_width=1, act_bit_width=1, in_bit_width=8, in_ch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_reshape = torch.load(\"xtrain25000014x14float32.pth\")\n",
    "ytrain_tensor = torch.load(\"ytrain250000int64.pth\")\n",
    "xval_reshape = torch.load(\"xval4412014x14float32.pth\")\n",
    "yval_tensor = torch.load(\"yval44120int64.pth\")\n",
    "xtest_reshape = torch.load(\"xtest4527014x14float32.pth\")\n",
    "ytest_tensor = torch.load(\"ytest45270int64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        #self.y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.X = X.unsqueeze(1)\n",
    "        self.y = y\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "val_data = Data(xval_reshape, yval_tensor)\n",
    "val_dataloader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(xtest_reshape, ytest_tensor)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from DatasetSplitByRounds import split_user_datasets_by_epochs, DataB\n",
    "\n",
    "Round_xtrain_datasets = split_user_datasets_by_epochs(xtrain_reshape,25)\n",
    "Round_ytrain_datasets = split_user_datasets_by_epochs(ytrain_tensor,25)\n",
    "\n",
    "\n",
    "# Initialize the cumulative dataset with the first 20 parts\n",
    "cumulative_xtrain_data = []\n",
    "cumulative_ytrain_data = []\n",
    "\n",
    "# Start with the first 20 parts\n",
    "for i in range(20):\n",
    "    cumulative_xtrain_data.append(Round_xtrain_datasets[i])\n",
    "    cumulative_ytrain_data.append(Round_ytrain_datasets[i])\n",
    "\n",
    "# Convert the lists of lists to NumPy arrays or PyTorch tensors before passing to the Data class\n",
    "Concat_xtrain_data = torch.cat(cumulative_xtrain_data, axis=0)  # Concatenate into a single array\n",
    "Concat_ytrain_data = torch.cat(cumulative_ytrain_data, axis=0)  # Concatenate into a single array\n",
    "train_data = Data(Concat_xtrain_data, Concat_ytrain_data)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataloader):\n",
    "    model.eval()\n",
    "    Tall_true_labels = []\n",
    "    Tall_predicted_labels = []\n",
    "\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for batch_idx, (xtest, ytest) in enumerate(test_dataloader):\n",
    "        xtest, ytest = xtest.to(device), ytest.to(device)\n",
    "        outputs = model(xtest)\n",
    "        batch_loss = criterion(outputs, ytest)\n",
    "        loss += batch_loss.item()\n",
    "        pred = outputs.data.argmax(1, keepdim=True)\n",
    "        correct += pred.eq(ytest.data.view_as(pred)).sum()\n",
    "        \n",
    "        total += len(ytest)\n",
    "    accuracy = 100. * correct.float() / total                   \n",
    "    loss = loss/total\n",
    "    return accuracy, loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from WeightInterpreter import WeightInterpreter\n",
    "\n",
    "# Create CSV to save results\n",
    "output_file = \"Results_CL_5Rounds/Accel6_training_testing_results.csv\"\n",
    "header = ['Run', 'Epoch', 'Train_Loss', 'Train_Accuracy', 'Validation_Loss', 'Validation_Accuracy', 'Test_Accuracy', 'Test_Loss']\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "\n",
    "# Initialize variables\n",
    "all_test_accuracies = []  # Store top-1 test accuracies for all runs\n",
    "T_epochs = 10\n",
    "num_epochs = 5\n",
    "num_runs = 6\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    #print(f\"========== Run {run} ==========\")\n",
    "    epoch_loss = []\n",
    "    batch_loss = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    if run== 1:\n",
    "        for epoch in range(T_epochs):\n",
    "            model.train()\n",
    "            criterion.train()\n",
    "\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_batch_loss = []\n",
    "\n",
    "            total_batches = len(train_dataloader)\n",
    "            progress_bar = tqdm(total=total_batches, desc=f\"Run {run} - Epoch {epoch+1}\", unit=\"batch\", position=0, leave=True)\n",
    "\n",
    "            for batch_idx, (xtrain, ytrain) in enumerate(train_dataloader):\n",
    "                xtrain, ytrain = xtrain.to(device), ytrain.to(device)\n",
    "                model_preds = model(xtrain)\n",
    "                _, pred_labels = torch.max(model_preds, 1)\n",
    "                train_correct += torch.sum(pred_labels == ytrain).item()\n",
    "                train_total += ytrain.size(0)\n",
    "                loss = criterion(model_preds, ytrain)\n",
    "                train_batch_loss.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.clip_weights(-1, 1)  # This line for DPFL\n",
    "\n",
    "                batch_loss.append(loss.item())\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix(batch=batch_idx + 1, refresh=True)\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss) / len(batch_loss))           \n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            criterion.eval()\n",
    "\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_batch_loss = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xval, yval) in enumerate(val_dataloader):\n",
    "                    xval, yval = xval.to(device), yval.to(device)\n",
    "                    val_model_preds = model(xval)\n",
    "                    val_loss = criterion(val_model_preds, yval)\n",
    "                    val_batch_loss.append(val_loss.item())\n",
    "                    _, val_pred_labels = torch.max(val_model_preds, 1)\n",
    "                    val_correct += torch.sum(val_pred_labels == yval).item()\n",
    "                    val_total += yval.size(0)\n",
    "\n",
    "            # Calculate average training and validation metrics\n",
    "            avg_train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "            train_accuracy = train_correct / train_total if train_total > 0 else 0.0\n",
    "            avg_val_loss = sum(val_batch_loss) / len(val_batch_loss)\n",
    "            val_accuracy = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "            #print(f\"Run {run} - Epoch {epoch + 1}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "            # Save metrics to CSV\n",
    "            with open(output_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([run, epoch + 1, avg_train_loss, train_accuracy * 100, avg_val_loss, val_accuracy * 100, None, None]) \n",
    "\n",
    "    else:\n",
    "        cumulative_xtrain_data.append(Round_xtrain_datasets[run+18])\n",
    "        cumulative_ytrain_data.append(Round_ytrain_datasets[run+18])\n",
    "        Concat_xtrain_data = torch.cat(cumulative_xtrain_data, axis=0)  # Concatenate into a single array\n",
    "        Concat_ytrain_data = torch.cat(cumulative_ytrain_data, axis=0)\n",
    "        train_data = Data(Concat_xtrain_data, Concat_ytrain_data)\n",
    "        train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            criterion.train()\n",
    "\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_batch_loss = []\n",
    "\n",
    "            total_batches = len(train_dataloader)\n",
    "            progress_bar = tqdm(total=total_batches, desc=f\"Run {run} - Epoch {epoch+1}\", unit=\"batch\", position=0, leave=True)\n",
    "\n",
    "            for batch_idx, (xtrain, ytrain) in enumerate(train_dataloader):\n",
    "                xtrain, ytrain = xtrain.to(device), ytrain.to(device)\n",
    "                model_preds = model(xtrain)\n",
    "                _, pred_labels = torch.max(model_preds, 1)\n",
    "                train_correct += torch.sum(pred_labels == ytrain).item()\n",
    "                train_total += ytrain.size(0)\n",
    "                loss = criterion(model_preds, ytrain)\n",
    "                train_batch_loss.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.clip_weights(-1, 1)  # This line for DPFL\n",
    "\n",
    "                batch_loss.append(loss.item())\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix(batch=batch_idx + 1, refresh=True)\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "            epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            criterion.eval()\n",
    "\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_batch_loss = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xval, yval) in enumerate(val_dataloader):\n",
    "                    xval, yval = xval.to(device), yval.to(device)\n",
    "                    val_model_preds = model(xval)\n",
    "                    val_loss = criterion(val_model_preds, yval)\n",
    "                    val_batch_loss.append(val_loss.item())\n",
    "                    _, val_pred_labels = torch.max(val_model_preds, 1)\n",
    "                    val_correct += torch.sum(val_pred_labels == yval).item()\n",
    "                    val_total += yval.size(0)\n",
    "\n",
    "            # Calculate average training and validation metrics\n",
    "            avg_train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "            train_accuracy = train_correct / train_total if train_total > 0 else 0.0\n",
    "            avg_val_loss = sum(val_batch_loss) / len(val_batch_loss)\n",
    "            val_accuracy = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "            #print(f\"Run {run} - Epoch {epoch + 1}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "            # Save metrics to CSV\n",
    "            with open(output_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([run, epoch + 1, avg_train_loss, train_accuracy * 100, avg_val_loss, val_accuracy * 100, None, None])\n",
    "        \n",
    "\n",
    "    torch.save(model.state_dict(), f\"Results_CL_5Rounds/Accel6_R{run}_model_state.pth\")     \n",
    "    # Testing part\n",
    "    model.eval()\n",
    "    test_accuracy, test_loss = test_inference(model, test_dataloader)  # Assuming this returns a dictionary with keys 'accuracy' and 'loss'\n",
    "#     test_accuracy = test_results['accuracy']\n",
    "#     test_loss = test_results['loss']\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Run {run} - Test Accuracy: {test_accuracy:.2f}%, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Save testing metrics to CSV\n",
    "    with open(output_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([run, None, None, None, None, None, test_accuracy, test_loss])\n",
    "    WeightInterpreter(model, \"Accel6\")\n",
    "# print(all_test_accuracies)        \n",
    "# Plot Top-1 Test Accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_runs + 1), all_test_accuracies, marker='o', linestyle='-', color='b')\n",
    "# plt.title(\"Top-1 Test Accuracy Across Runs\")\n",
    "plt.xlabel(\"Training Cycles (Each Cycle: 5 Epochs)\")\n",
    "plt.ylabel(\"Top-1 Test Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Results_CL_5Rounds/Accel6_test_accuracies_plot.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
